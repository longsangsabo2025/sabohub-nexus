/**
 * AI Service - Real AI Integration
 * Elon Musk approach: Make it work, then make it better
 * 
 * Supports:
 * - OpenAI GPT-4 (cloud, best quality)
 * - Ollama (local, free, privacy)
 * - Gemini (Google, fast)
 */

interface AIMessage {
  role: 'system' | 'user' | 'assistant';
  content: string;
}

interface AIResponse {
  content: string;
  confidence: number;
  tokens_used?: number;
}

interface CompanyContext {
  tasks: {
    total: number;
    overdue: number;
    completed: number;
    completion_rate: number;
  };
  team: {
    total: number;
    active: number;
  };
  recent_issues: string[];
}

class AIService {
  private provider: 'openai' | 'ollama' | 'gemini' | 'mock';
  private apiKey?: string;
  private baseUrl: string;
  private model: string;

  constructor() {
    // Check environment for AI config
    this.provider = this.detectProvider();
    this.apiKey = this.getApiKey();
    this.baseUrl = this.getBaseUrl();
    this.model = this.getModel();
  }

  private detectProvider(): 'openai' | 'ollama' | 'gemini' | 'mock' {
    if (import.meta.env.VITE_OPENAI_API_KEY) return 'openai';
    if (import.meta.env.VITE_GEMINI_API_KEY) return 'gemini';
    if (import.meta.env.VITE_OLLAMA_URL) return 'ollama';
    return 'mock'; // Fallback to rule-based
  }

  private getApiKey(): string | undefined {
    switch (this.provider) {
      case 'openai':
        return import.meta.env.VITE_OPENAI_API_KEY;
      case 'gemini':
        return import.meta.env.VITE_GEMINI_API_KEY;
      default:
        return undefined;
    }
  }

  private getBaseUrl(): string {
    switch (this.provider) {
      case 'openai':
        return 'https://api.openai.com/v1';
      case 'ollama':
        return import.meta.env.VITE_OLLAMA_URL || 'http://localhost:11434';
      case 'gemini':
        return 'https://generativelanguage.googleapis.com/v1'; // Stable v1 API
      default:
        return '';
    }
  }

  private getModel(): string {
    switch (this.provider) {
      case 'openai':
        return import.meta.env.VITE_OPENAI_MODEL || 'gpt-4-turbo-preview';
      case 'ollama':
        return import.meta.env.VITE_OLLAMA_MODEL || 'llama2';
      case 'gemini':
        return 'gemini-2.5-flash'; // Latest: Fast, intelligent, best price-performance
      default:
        return 'mock';
    }
  }

  /**
   * Generate AI insights from company data
   * Elon's principle: AI should predict problems BEFORE they escalate
   */
  async generateInsights(context: CompanyContext): Promise<AIResponse> {
    const systemPrompt = `You are an AI business analyst for a CEO. Analyze company metrics and provide actionable insights.

Current Context:
- Tasks: ${context.tasks.total} total, ${context.tasks.overdue} overdue, ${context.tasks.completed} completed (${context.tasks.completion_rate}% rate)
- Team: ${context.team.total} members, ${context.team.active} active
- Recent Issues: ${context.recent_issues.join(', ')}

Focus on:
1. Critical risks (overdue tasks, bottlenecks)
2. Opportunities (high completion rates, trends)
3. Anomalies (sudden changes)
4. Actionable recommendations

Be concise. Vietnamese language. CEO-level insights only.`;

    const userPrompt = `Ph√¢n t√≠ch t√¨nh h√¨nh hi·ªán t·∫°i v√† ƒë∆∞a ra 3-5 insights quan tr·ªçng nh·∫•t.`;

    return this.chat([
      { role: 'system', content: systemPrompt },
      { role: 'user', content: userPrompt },
    ]);
  }

  /**
   * Chat with AI - natural language queries
   */
  async chat(messages: AIMessage[]): Promise<AIResponse> {
    try {
      switch (this.provider) {
        case 'openai':
          return await this.chatOpenAI(messages);
        case 'ollama':
          return await this.chatOllama(messages);
        case 'gemini':
          return await this.chatGemini(messages);
        default:
          return this.mockChat(messages);
      }
    } catch (error) {
      console.error('AI Service Error:', error);
      return this.mockChat(messages); // Fallback to mock
    }
  }

  private async chatOpenAI(messages: AIMessage[]): Promise<AIResponse> {
    const response = await fetch(`${this.baseUrl}/chat/completions`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json',
        Authorization: `Bearer ${this.apiKey}`,
      },
      body: JSON.stringify({
        model: this.model,
        messages,
        temperature: 0.7,
        max_tokens: 500,
      }),
    });

    if (!response.ok) {
      throw new Error(`OpenAI API error: ${response.statusText}`);
    }

    const data = await response.json();
    return {
      content: data.choices[0].message.content,
      confidence: 0.95,
      tokens_used: data.usage?.total_tokens,
    };
  }

  private async chatOllama(messages: AIMessage[]): Promise<AIResponse> {
    const response = await fetch(`${this.baseUrl}/api/chat`, {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({
        model: this.model,
        messages,
        stream: false,
      }),
    });

    if (!response.ok) {
      throw new Error(`Ollama API error: ${response.statusText}`);
    }

    const data = await response.json();
    return {
      content: data.message.content,
      confidence: 0.85,
    };
  }

  private async chatGemini(messages: AIMessage[]): Promise<AIResponse> {
    // Convert messages to Gemini format
    const contents = messages
      .filter((m) => m.role !== 'system')
      .map((m) => ({
        role: m.role === 'assistant' ? 'model' : 'user',
        parts: [{ text: m.content }],
      }));

    const response = await fetch(
      `${this.baseUrl}/models/${this.model}:generateContent?key=${this.apiKey}`,
      {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ contents }),
      }
    );

    if (!response.ok) {
      const errorText = await response.text();
      console.error('Gemini API Error:', response.status, errorText);
      throw new Error(`Gemini API error: ${response.status} ${response.statusText}`);
    }

    const data = await response.json();
    
    // Safety check
    if (!data.candidates || !data.candidates[0] || !data.candidates[0].content) {
      console.error('Invalid Gemini response:', data);
      throw new Error('Invalid response from Gemini API');
    }
    
    return {
      content: data.candidates[0].content.parts[0].text,
      confidence: 0.9,
    };
  }

  /**
   * Mock AI - Rule-based fallback (current implementation)
   * Fast, free, works offline
   */
  private mockChat(messages: AIMessage[]): AIResponse {
    const lastMessage = messages[messages.length - 1].content.toLowerCase();

    // Pattern matching for common queries
    if (lastMessage.includes('qu√° h·∫°n') || lastMessage.includes('overdue')) {
      return {
        content: `üö® **C·∫£nh b√°o: C√≥ nhi·ªám v·ª• qu√° h·∫°n!**

Hi·ªán t·∫°i c√≥ nhi·ªÅu tasks ƒë√£ qu√° deadline c·∫ßn x·ª≠ l√Ω ngay:
- ∆Øu ti√™n URGENT tr∆∞·ªõc
- Review l·∫°i ph√¢n c√¥ng c√¥ng vi·ªác
- C√¢n nh·∫Øc redistribute workload

**Action c·ª• th·ªÉ:**
1. H·ªçp team ngay h√¥m nay
2. Re-prioritize tasks
3. Th√™m resources n·∫øu c·∫ßn`,
        confidence: 0.8,
      };
    }

    if (lastMessage.includes('ph√¢n t√≠ch') || lastMessage.includes('t√¨nh h√¨nh')) {
      return {
        content: `üìä **Ph√¢n t√≠ch t·ªïng quan:**

**ƒêi·ªÉm m·∫°nh:**
- Team ƒëang c√≥ momentum t·ªët
- Nhi·ªÅu tasks completed g·∫ßn ƒë√¢y

**ƒêi·ªÉm y·∫øu:**
- Overdue tasks tƒÉng ‚Üí c·∫ßn attention
- Completion rate c√≥ th·ªÉ c·∫£i thi·ªán

**Khuy·∫øn ngh·ªã:**
1. Focus v√†o tasks qu√° h·∫°n tr∆∞·ªõc
2. Optimize workflow ƒë·ªÉ tr√°nh bottleneck
3. Set up alerts t·ª± ƒë·ªông cho deadline`,
        confidence: 0.75,
      };
    }

    if (lastMessage.includes('doanh thu') || lastMessage.includes('revenue')) {
      return {
        content: `üí∞ **Ph√¢n t√≠ch t√†i ch√≠nh:**

Hi·ªán t·∫°i ch∆∞a c√≥ ƒë·ªß d·ªØ li·ªáu financial ƒë·ªÉ ph√¢n t√≠ch chi ti·∫øt.

**C·∫ßn l√†m:**
- T√≠ch h·ª£p financial_transactions table
- Track revenue theo th·ªùi gian th·ª±c
- Set up KPI tracking

C√≥ mu·ªën t√¥i setup financial tracking kh√¥ng?`,
        confidence: 0.6,
      };
    }

    // Default response
    return {
      content: `T√¥i ƒë√£ ph√¢n t√≠ch request c·ªßa b·∫°n. Hi·ªán t·∫°i t√¥i c√≥ th·ªÉ gi√∫p:

1. **Ph√¢n t√≠ch tasks** - overdue, completion rates
2. **Team insights** - productivity, workload
3. **Alerts** - critical issues c·∫ßn attention

ƒê·ªÉ ph√¢n t√≠ch s√¢u h∆°n, h√£y cho t√¥i bi·∫øt b·∫°n mu·ªën focus v√†o aspect n√†o?`,
      confidence: 0.7,
    };
  }

  /**
   * Get provider info for UI display
   */
  getProviderInfo() {
    return {
      provider: this.provider,
      model: this.model,
      status:
        this.provider === 'mock'
          ? 'Rule-based (Free)'
          : this.provider === 'ollama'
            ? 'Local AI (Free)'
            : 'Cloud AI',
    };
  }
}

// Singleton instance
export const aiService = new AIService();
export type { AIMessage, AIResponse, CompanyContext };
